{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS Industry Blueprints for Data &amp; AI","text":""},{"location":"#what-is-aws-industry-blueprints-for-data-ai","title":"What is AWS Industry Blueprints for Data &amp; AI?","text":"<p>AWS Industry Blueprints for Data &amp; AI offers a collection of building <code>components</code>, including CDK code modules and solution accelerators to facilitate the configuration and deployment of tailored components for various industry verticals' data-to-insights needs. Users can swiftly set up foundational data ingestion pipeline or choose customized add-on components for specific use cases. Additionally, the Blueprints incorporate AWS technology partner ecosystem products with proprietary capabilities to meet specific requirements. This project simplifies the process of building industry solutions by providing pattern-based architectural definitions that developers can easily leverage.</p> <p>The add-on components provided by Industry Blueprints for Data &amp; AI are designed for industry-specific use cases, such as <code>Churn Prediction</code>, <code>Campaign Management</code>, and <code>ID Resolution</code>, while also addressing common tasks like <code>Deploying BI Dashboards</code> and <code>Data Governance</code>. These components can be easily combined with minimal adjustments to create comprehensive industry solutions, complete with business logic. Each component is pluggable yet generic enough to be reused for specific vertical solutions. The Blueprints components encapsulate various AWS Data &amp; AI services, including <code>Amazon AppFlow</code>, <code>EMR</code>, <code>Kinesis</code>, <code>RDS</code>, <code>Redshift</code>, <code>DynamoDB</code>, <code>Glue</code>, <code>Athena</code>, as well as purpose-built services for specific industries like <code>Amazon HealthLake</code>, <code>Amazon Security Lake</code>, <code>AWS Supply Chain</code> and etc. <code>AWS Solutions Constructs</code> serve as building blocks for the CDK code modules, offering well-architected patterns that allow developers to define solutions in code, resulting in predictable and repeatable infrastructure.</p>"},{"location":"#why-use-aws-industry-blueprints-for-data-ai","title":"Why use AWS Industry Blueprints for Data &amp; AI?","text":"<p>The following diagram presented illustrates the complete process of <code>Turn Data into Insights</code>, which begins at the bottom and culminates at the top. </p> <p></p> <p>Regardless of the industry, the initial stage of the customers' data journey is similar. Data is gathered from multiple sources and funneled into the AWS Lake House. Subsequently, the data is processed and utilized according to specific use cases. </p> <p>As part of the Industry Blueprints for Data &amp; AI framework, we aim to break down this data journey into two main components categories: a foundational <code>data ingestion pipeline</code> and specialized <code>use case modules</code> that are tailored to each industry's unique needs. These modules concentrate on processing and utilizing data in industry-specific ways, adding a distinct flavor to the process. As we can see from the righthand side of the diagram, the framework also encompasses technology partner ecosystem products, which may provide customers with exclusive capabilities. As depicted on the top right side, AWS industry-specific services, built with a specific purpose in mind, can act as catalysts for accelerating solution building. On the bottom left are our partners who offer industry-specific solutions built on AWS data and analytics services. </p>"},{"location":"comingsoon/","title":"Coming soon","text":"<p>Thanks for your patience.</p>"},{"location":"components/","title":"Industry Blueprints for Data &amp; AI Components","text":""},{"location":"components/#challenges-of-package-development-for-industry-solutions","title":"Challenges of package development for industry solutions","text":"<p>The challenges of package development for industry solutions encompass factors such as the potential short lifespan of solutions, the need to adapt to new AWS services and features, the effort required to update older solutions, ensuring solution effectiveness, providing an acceleration framework for partners and customers, and addressing the design and development demands of multi-vertical solutions. Overcoming these challenges requires a combination of agility, innovation, and a deep understanding of industry needs.</p>"},{"location":"components/#componentized-building-mechanism","title":"Componentized building mechanism","text":"<p>As shown in the above diagram, our process for creating end-to- end solutions using Industry Blueprints begins with a foundational data ingestion pipeline. From there, we incorporate add-on modules that include comprehensive business logic. Each of these components is designed to be <code>pluggable</code> and generic enough to cater to the specific needs of various vertical solutions. The components of the blueprints encapsulate existing AWS Data and AI services. Our approach to building solutions is similar to putting together <code>LEGO</code> blocks, and as we like to say, the possibilities are limitless. </p>"},{"location":"components/#how-to-scale-with-componentized-building-mechanism","title":"How to scale with componentized building mechanism","text":"<p><code>Industry Blueprints for Data &amp; AI</code>offer a wide range of components designed to cater to both industry-specific use cases, and Data Ingestion with proprietary industry standards.</p> <p>To give you a clearer picture, we have color-coded these components in the following diagram. Orange represents cross-industry reusable components that can be utilized in various sectors. On the other hand, yellow, green, and blue components signify those with specific industry flavors, tailored to meet the unique requirements of different industries. These components can be seamlessly assembled together to create a comprehensive, runnable industry solution complete with business logic. </p> <p></p>"},{"location":"components/#what-benefit-does-componentized-building-mechanism-provide","title":"What Benefit does componentized building mechanism provide?","text":"<ul> <li> <p>Bring partners increased agility, efficiency, developer velocity, and lower maintenance effort with reusable cross-industry components</p> </li> <li> <p>Leverage cross-industry applicable base components with use case oriented add-ons to accommodate faster change and solution adaptability. This can simplify the sourcing, implementation and integration process.</p> </li> </ul>"},{"location":"core_concepts/","title":"Core Concepts","text":"<p>This document provides a high level overview of the Core Concepts that are embedded in the <code>Industry Blueprints for Data &amp; AI</code> framework.</p> Concept Description Data Ingestion Pipeline Collect raw data from diverse sources and channel it into the AWS Lake House architecture. Add-on Use Case Concentrate on processing and utilizing data in industry-specific ways. Blueprint A <code>blueprint</code> combines <code>data ingestion pipeline</code> and <code>add-on use case</code> modules into an end-to-end industry solution with business logic. <p>Our dedicated team ensures that the artifacts are regularly updated, reflecting the latest advancements and best practices in the data and AI landscape.</p> <p>The following solution architecture diagram is an example of an end-to-end <code>Customer 360</code> solution built using <code>Industry Blueprints for Data &amp; AI</code>.</p> <p></p> <p>The dashed boxes in this diagram represent <code>add-on use case</code> components that address each use case in the solution, while the remaining elements form the <code>data ingestion pipeline</code>. </p> <p>This sample end-to-end <code>Customer 360</code> solution built with Blueprints showcases several key features, including: </p> <ul> <li>Low Code / No Code Data Ingestion: Simplifying data ingestion to make it accessible to users with varying levels of technical expertise. </li> <li>SQL-oriented Data Pipelines: Facilitating efficient data  processing and transformation. </li> <li>Visual &amp; SQL-Oriented Analytics, ML &amp; BI in Redshift:  Enabling powerful analytics, machine learning, and  business intelligence capabilities. </li> <li>Modular Architecture: Allowing for easy  customization and adaptation of the solution to  specific use cases. </li> <li>Scalability &amp; Elasticity: Ensuring that the solution can  grow and adapt with the changing needs of the business. </li> </ul> <p>This approach simplifies the integration of common, well-architected patterns to achieve solution goals, making it easier for partners to create tailored solutions for their end customers\u2019 specific needs. </p>"},{"location":"overview_addon/","title":"Add-on Use Case Components","text":"<p>The <code>add-on use case</code> components provided by Industry Blueprints for Data &amp; AI concentrate on processing and utilizing data in industry-specific ways, adding a distinct flavor to the process. They are designed for industry-specific use cases, such as <code>Churn Prediction</code>, <code>Campaign Management</code>, and <code>ID Resolution</code>, while also addressing common tasks like <code>Deploying BI Dashboards</code> and <code>Data Governance</code>. These components can be easily combined with minimal adjustments to create comprehensive industry solutions, complete with business logic. </p> <p>AWS offers a variety of purpose-built services for specific industries such as <code>Amazon HealthLake</code>, <code>Amazon Security Lake</code>, <code>AWS Supply Chain</code>, etc. These services enable organizations to accelerate their digital transformation journey, drive innovation, and gain a competitive edge in their respective industries.</p> <p>The <code>Add-on Use Case Components</code> section within <code>AWS Industry Blueprints for Data &amp; AI</code> provides a collection of ready-to-deploy modules including AWS ISV partner products. </p>"},{"location":"overview_blueprints/","title":"Industry Blueprints","text":"<p>A blueprint combines data ingestion pipeline and add-on use case modules into an end-to-end industry solution with business logic.</p> <p>The <code>Example Blueprints</code> section includes examples of an end-to-end solution built leveraging <code>Industry Blueprints for Data &amp; AI</code> in the following industry verticals:</p> <ul> <li>Customer 360</li> <li>Supply Chain</li> <li>Healthcare &amp; Life Science</li> <li>Manufacture &amp; Automotive</li> <li>Retail &amp; Advertising</li> <li>Media &amp; Entertainment</li> <li>Financial Services Industry</li> <li>Gaming</li> <li>Sustainability</li> </ul> <p>We expect to add more verticals as more components get available.</p>"},{"location":"overview_ingestion/","title":"Data Ingestion Pipeline Components","text":"<p>Data ingestion is the process of transporting data from one or more sources to a target storage medium for further processing and analysis.</p> <p>Regardless of the industry, the initial stage of AWS customers' data journey is similar - data is gathered from multiple sources and funneled into the AWS Lake House. The <code>data ingestion pipeline</code> is the component which collects raw data from diverse sources and channel it into the AWS Lake House architecture. </p> <p>As data ingestion methods, AWS provides a variety of services and capabilities to ingest different types of data into your data lake house including <code>Amazon AppFlow</code>, <code>Kinesis Data Firehose</code>, <code>AWS Snow Family</code>, <code>Glue</code>, <code>DataSync</code>, <code>AWS Transfer Family</code>, <code>Storage Gateway</code>, <code>Direct Connect</code>, <code>Database Migration Service (DMS)</code>, etc. </p> <p>But data ingestion can be a complex process due to various factors such as the type and source of the data, the method of ingestion (batch or real-time), data volume, and industry standards. The data may come from on-premises storage platforms like legacy data servers, mainframes, or data warehouses, as well as SaaS platforms, and may be structured or unstructured (e.g. images, text files, audio and video, and graphs). These factors require careful consideration during the design and implementation of data ingestion pipelines to ensure efficient and reliable transfer of data to the AWS Lake House.</p> <p>The <code>Data Ingestion Pipeline Components</code> section within <code>AWS Industry Blueprints for Data &amp; AI</code> offers a curated collection of ready-to-deploy modules including AWS ISV partner products, providing a head start in building robust data ingestion pipelines. These components are designed to streamline the process and accelerate development, enabling organizations to quickly establish efficient data pipelines. Furthermore, some of these components are tailored to meet industry-specific standards, ensuring compliance and alignment with regulations and requirements specific to sectors such as healthcare, finance, and more. With these deployment-ready modules, developers can expedite the creation of reliable data ingestion pipelines, saving time and effort in the solution development journey.</p>"},{"location":"request/","title":"Request for a New Component","text":"<p>To post feedback, submit feature ideas, or report bugs regarding any component, use the <code>Issues</code> section of this GitHub repo, and tag it with <code>component</code>.</p>"},{"location":"trainings/","title":"Trainings","text":"<p>We will provide a series of trainings in the format of webinars on each individual modules, and how to leverage Industry Blueprints for Data &amp; AI to create end-to-end industry solutions.</p> <p>To post feedback, submit feature ideas, or report bugs regarding the workshop, use the <code>Issues</code> section of this GitHub repo, and tag it with <code>training</code>.</p>"}]}